\documentclass[12pt,abstract=true]{scrartcl}

%<<< Preamble
\usepackage{tikz}
\usepackage{array}
\usepackage{pifont}
\usepackage{engord}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{multicol}
\usepackage{pgfplots}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[plain]{fancyref}
\usepackage[unicode]{hyperref}
\usepackage{graphicx,tabularx}
\usepackage[shortlabels]{enumitem}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage{amsmath,amssymb,amsthm,amscd,amstext}
\usepackage{mathtools}
\usepackage{exscale}
\usepackage{relsize}
\usepackage{mathrsfs}
\usepackage{tocloft}

\usepackage{setspace}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage{chngpage}

\usepackage{lmodern}
\usepackage{mathpazo}

\pgfplotsset{compat=1.6}
\usetikzlibrary{arrows}
\usetikzlibrary{patterns}
\usetikzlibrary{positioning}
\usetikzlibrary{automata}
\usepackage{tikz-3dplot}

\tikzset{ alert/.style={ very thick, draw=red!80, fill=red!20 } }
\tikzset{ fancy/.style={ very thick, draw=blue!80, fill=blue!20 } }
\tikzset{
  dot/.style={
    very thick,circle,draw=black,fill=black,inner sep=0,minimum size=5pt
  }
}
\tikzset { every state/.style={fancy} }
\tikzset {
  general/.style = {
    shorten >=2pt, node distance=2.5cm, on grid, thick,>=stealth', auto
  }
}
\newcommand{\mathhuge}[1]{\mathlarger{\mathlarger{\mathlarger{#1}}}}
\newcommand{\mathtiny}[1]{\mathsmaller{\mathlarger{\mathlarger{#1}}}}

\renewcommand\cftsecfont{\normalsize}
\renewcommand\cftsecpagefont{\normalsize}
\renewcommand\cftsecafterpnum{\par}
\setlength{\cftbeforesecskip}{0.2em}

\renewcommand\cftsubsecfont{\small}
\renewcommand\cftsubsecpagefont{\small}
\renewcommand\cftsubsecafterpnum{\par}

\newcommand\upper[1]{\textsuperscript#1}
\numberwithin{equation}{section}
\mathtoolsset{centercolon}
\bibliographystyle{plain}

% Theorem environment <<<
\theoremstyle{definition}   \newtheorem{definition}{Definition}[section]
\theoremstyle{plain}        \newtheorem{theorem}{Theorem}[section]
\theoremstyle{plain}        \newtheorem{observation}{Observation}[section]
\theoremstyle{plain}        \newtheorem{fact}{Fact}[section]
\theoremstyle{plain}        \newtheorem{claim}{Claim}[section]
\theoremstyle{plain}        \newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{plain}        \newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{remark}       \newtheorem{example}{Example}[section]
\theoremstyle{remark}       \newtheorem{remark}{Remark}[section]
\newcommand*{\fancyrefdeflabelprefix}{def}
\newcommand*{\fancyrefthmlabelprefix}{thm}
\newcommand*{\fancyreflemlabelprefix}{lem}
\newcommand*{\fancyrefcorlabelprefix}{cor}
\newcommand*{\fancyrefalglabelprefix}{alg}
\newcommand*{\fancyreflnlabelprefix}{ln}
\frefformat{plain}{\fancyreflnlabelprefix}{line #1}
\Frefformat{plain}{\fancyreflnlabelprefix}{Line #1}
\frefformat{plain}{\fancyrefalglabelprefix}{algorithm (#1)}
\Frefformat{plain}{\fancyrefalglabelprefix}{Algorithm (#1)}
\frefformat{plain}{\fancyrefdeflabelprefix}{definition (#1)}
\Frefformat{plain}{\fancyrefdeflabelprefix}{Definition (#1)}
\frefformat{plain}{\fancyrefthmlabelprefix}{theorem (#1)}
\Frefformat{plain}{\fancyrefthmlabelprefix}{Theorem (#1)}
\frefformat{plain}{\fancyreflemlabelprefix}{lemma (#1)}
\Frefformat{plain}{\fancyreflemlabelprefix}{Lemma (#1)}
\frefformat{plain}{\fancyrefcorlabelprefix}{corollary (#1)}
\Frefformat{plain}{\fancyrefcorlabelprefix}{Corollary (#1)}
%>>>
% Alter some LaTeX Float for better treatment of figures: <<<
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
\renewcommand{\topfraction}{0.9}	% max fraction of floats at top
\renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}     % 2 may work better
\setcounter{dbltopnumber}{2}    % for 2-column pages
\renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
\renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
\renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
\renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

\allowdisplaybreaks[4]

%>>>
%>>>

\begin{document}

\begin{center} %<<< Title
	\Large \textbf{\textsf{
		Metrics and Algorithms for Evolving Social Networks}} \\[1.2em]
	\normalsize 
		Pufan He\upper{1}, Yichao Zhou\upper{2}, Qiwei Feng\upper{3},
		Yu Yan\upper{4}, Qingyang Li\upper{5} and Xin Xing\upper{6}\\[1em]
	\small
	\begin{tabular}{*{3}{>{\centering}p{.3\textwidth}}}
		\upper{1}\small\href{mailto:hpfdf@126.com}{hpfdf@126.com} &%
		\upper{2}\href{mailto:broken.zhou@gmail.com}{broken.zhou@gmail.com} &%
		\upper{3}\href{mailto:gdfqw93@163.com}{gdfqw93@163.com} \tabularnewline
		\upper{4}\href{mailto:whyvine@hotmail.com}{whyvine@hotmail.com} &
		\upper{5}\href{mailto:591527324@qq.com}{591527324@qq.com} &
		\upper{6}\href{mailto:823291634@qq.com}{823291634@qq.com}
	\end{tabular} \\[1em]

	\small Institute for Interdisciplinary Information Sciences,
	Tsinghua University \\[1.5em]
\end{center} \par %>>>

\begin{abstract}
We address the dynamic properties of typical social networks, in which person
is considered as vertex while relation is considered as edge. We suppose the
evolution of a social network overtime only includes bidirectional new
connections between existing or newly created nodes, and we assume the graph
will evolve as a typical social network. We study the activity and centrality
metrics for nodes in such graphs by regulate a series of reasonable properties
the metrics should satisfy. We then formally define our metrics and give their
mathematical analysis. We have developed a practical based framework to
maintain the evolving large-scale network and the algorithm to compute our
metrics with preferable time and space consumption, including historical
queries. Finally, we put the result of experiment with a couple of authentic
data.

\smallskip
\noindent \textbf{Keywords:} social network, dynamic graph, vertex activity,
vertex centrality, online algorithm.
\end{abstract}
\tableofcontents
\newpage

\section{Introduction}
Social networks is a surprising interdisciplinary field of social science and
computer science, allowing helpful analysis of large-scaled social behavior
and tendency. In this paper, we will take more information of social networks,
the time dimension, and make the similar study to the social behaviors. We
build metrics for evolving social networks and design the algorithm to
calculate them. We compare how different the outcome by our method would be
against the classical analysis with un-timestamped data mining.

\subsection{Dynamic Metrics}
Networks contain too much information to be easily observed by people without
any abstraction. To study a network, people exploited the basic form of
quantification to concentrate on specific features of the whole graph, or an
individual vertex. Our work is to promote this methodology to the dynamic
graphs. We basically design the new dynamic metrics according to existing
successful static ones, but we will point out a important metric that can be
only observed in time evolving networks, the activity. Intuitively speaking,
a vertex with high activity indicates more frequent recent social actions, and
the vertex is more willing to make new change by the structural sense of graph.
The accuracy of activity will be quite vital to recommendation algorithms of
advertisement, new friends, or search results. Activity can be also used by
other metrics, so that the computation of other metrics will no longer need the
time information directly, but referring to the activity instead.

Centrality, usually defined on vertices, is frequently used to measure the
importance of a specific node in graphs of all kinds. There are many previous
studies about the static graph centralities[?], such as page rank
algorithm\cite{page1999pagerank}, and there are many useful
network analysis algorithms that used some kind of graph centralities. However,
most social networks change over time, and even more unfortunately, some of the
networks are large-scaled, which could make the analysis become hard and
costly, hence limit the scale of meaningful data mining. Moreover, the time
effect can also become a factor of the centrality, that is, the same graph with
different evolving sequence may have totally different metrics. On an
intuitive level, the node connected with more higher activity nodes should be
more important at that time, even if other node have much more silent
neighbors. If we do not excavate the information on the time dimension, we will
lose tremendous information and also the possibility of accumulative
computation which could be helpful in enhancement the speed of answering
consistent quering.

In Section 3 we will concentrate on centrality and activity in evolving social
network. We will define what centrality or activity is good, and we will design
some instance of those metrics by the standard we set. In Section 4, we will
give the algorithms to compute our designed centrality and activity from the
online data structure which maintains the evolving social network. We will also
show how to quickly locate significant vertices that has high activity or
centrality at a specific time. In Section 5, we will show the metrics make
sense with real world data.

\subsection{Algorithms}
Graph theory is very successful on storing and solving variant problems on
static networks, we have sophisticated methodology on finding the shortest
paths, stratch a minimal spanning tree, finding the maximal $S$-$T$ flow, etc.
And we also have a well-established theoretical system to study problems that
do not seems to have perfect solutions. However, dynamic online queries can be
very hard for general graphs. Best known solutions for many dynamic basic graph
problems still remain unsatisfiable for general demands. So to design metrics
for an general evolving network is a very challenging and open problem. We
concentrate on the popular studied social networks and try to create meaningful
and available approaches of operation on such networks.

In Section 2 we will discuss the known properties of a typical social network,
which could be helpful for our algorithm designs. In Section 4, we will
establish a framework of data structure and algorithms to solve series of
possible queries including computing the metrics about a evolving social
network.

\section{Evolving Social Network}
In some related research on evolving graphs, the time dimension is represented
by constructing a sequence of graphs $EG=\{G_t(V_t,E_t)\}$ on different time
spots $t$. This model is highly expressive and universal in mathematical
statement, thus we adopt in. However, we constrain the relation of
time-adjacent graphs in order to simplify the analysis and remain the highest
possible loyalty to the reality. We build the \textbf{pure growing model},
which care about the new connection forming among vertices, neglecting the
anti-growing operations such as edge deletions. We assume the existence of
every connection will contribute to the metrics, even if the connection is
later erased, so we do not delete edges or nodes in our model, instead we
could add an decay mechanism to reduce the affect of old formed links, which
will be discussed in the next section. Note that in pure growing model, we
will assign every edge a timestamp, namely when the edge is created. And thus
there could be more than one edges between a pair of nodes, with different
timestamp.

\subsection{Definition}
\definition A \textbf{pure growing model} for an evolving graph $EG$ is defined
in pair \begin{equation}
EG=(V,E),
\end{equation}
where $V$ is the set of all nodes in the network, and $E$
is the set of edges, formally an edge $e\in E$ for $e=(u,v,t)$,
where $u,v\in V$ are the two end points of the edge, $u<v$ (to ensure
bidirectional), $t\in \mathbb{N}$. The $t$ component stored the forming time of
the graph. As for weighted evolving graph model, we modifies $e\in E$ for
$e=(u,v,t,w)$, where $w\in\mathbb{R}^+$ stands for the multiplicity of the
edge, which might be useful in some circumstances, and the others stay the same
meaning. In the unweighted case, we can simply set $w=1$ for all edges.

\definition The historical edge set $E_t$
\begin{equation}
E_t=\{e\;|\;e(u,v,t',w)\in E\text{ and }t'\leq t\}.
\end{equation}
An immediate statement one can ensure is
\begin{equation}
\forall t, E_t\subseteq E\text{ and }p\leq q\implies E_p\subseteq E_q.
\end{equation}

Define $G_t$ to be the graph formed by $V$ and $E_t$. So $G_t$ is the snapshot
of the whole network at time $t$.

Now we define the update of an evolving graph in our model. First we assume all
possible active vertices has been already defined, so $V$ does not change.
\definition 
$EG^*=(V,E^*)$ where $E\subseteq E^*$. We assume the edges are added to the
model by correct time order, so
\begin{equation}
\forall e\in E^*\setminus E, \forall e'\in E, e=(u,v,t,w)\text{ and }
e'=(u',v',t',w'), t\geq t'.
\end{equation}

And we define the notation of some frequently used basic functions:

\definition The adjacency matrix.
\begin{equation}
A(G_t)=\begin{pmatrix}a_{ij}\end{pmatrix}_{n\times n},\text{ where }n=|V|,
a_{ij}\in \mathbb{R}^+,
\end{equation}
is the adjacency matrix of the graph at time $t$. More detailed
\begin{equation}
a_{ij}=\begin{cases}
0&(i=j)\\
\sum_{e\in E_t\text{ and }e=(i,j,t,w)}w &(i<j)\\
a_{ji}&(i>j)
\end{cases}
\end{equation}


\begin{definition}
The degree function $k(G_t,v)$.
\begin{equation}
k(G_t,v)=\sum_{\mathclap{e\in E_t, e=(x,y,t,w)}}w(1-\delta(x,v)\delta(y,v)),
\end{equation}
where \begin{equation}
\delta(x,y)=\begin{cases}1&(x\neq y)\\0&(x=y)\end{cases}.
\end{equation}
\end{definition}

\subsection{Properties}
In most social networks, people have their own friendship circles, and the
average circle size is quite limited because people do less interaction with
strangers than their close friends or family members. We call this phenomenon
the \textbf{sparsity} of social network.
\definition Reduced edge set (historical) $E_t^\bullet=\{(x_i,y_i)\}$:
\begin{equation}
(x,y)\in E_t^\bullet \iff \exists e(x,y,t,w)\in E_t.
\end{equation}
\definition Reduced degree $k^\bullet(G_t,v)$:
\begin{equation}
k^\bullet(G_t,v)=\#\{u\;|\;(u,v)\in E_t^\bullet\text{ or }(v,u)\in
E_t^\bullet\}.
\end{equation}

We then have
\begin{equation}
\left<k^\bullet\right>=2\left|E^\bullet\right|/\left|V\right|,
\end{equation}
where $\left|E^\bullet\right|/\left|V\right|$ only depends on the structure of
the network at a time spot. In many available online datasets, such as [?]%TODO
we can assume $\left<k^\bullet\right>$ is limited by a constant $K$, typically
$10^2$.
\definition Sparcity constant $K=\left<k^\bullet\right>$.

\begin{observation} $K=O(1)$.
\end{observation}
So if we only use the reduced edge list in our framework as additional memory
requirements, or similarly speaking, consider the adjacency matrix to be a
sparse matrix and only store the non-zero entries, the space expense would then
be just $O(n)$, which is highly available. However this does not means we can
handle neighbor updating operations in $O(1)$ time, because
\begin{equation}\max k^\bullet\geq
\left<k^\bullet\right>=K \end{equation} and the former one may be very large.
We cannot trivially assert the amortized time complexity to be $O(1)$ neither,
because the nodes with high degrees may requires more operations over time,
just like the famous friendship paradox[?].

Another famouse property of social networks is known as the small world
effect[?], for that the giant component contains nearly all active nodes and it
has very small diameter.

The clustering coefficient[?], namely
\begin{equation}
c.c=\frac{\#\text{closed paths of len 2}}{\#\text{paths of len 2}},
\end{equation}
for a typical social network is a possitive
number, which means people are more willing to make interactions with close
related people than unfamiliars. 

\section{Activity and Centrality}

\subsection{Desiderata}
\definition $Act(G_t,v)$ returns the activity of node $v$ in the evolving graph in
time $t$. Then $Act(G_t,v)$ should be a deterministic function of $v$ and
$E_t$.
\begin{itemize}
\item \textbf{normalization}:

We want the metrics to derive other functions, so it will be nice if we can
limit the range of the function.
\begin{equation}
0\leq Act(G_t,v) \leq 1.
\end{equation}

\item \textbf{initialization}:

A node with no neighbors is totally inactive.
\begin{equation}
k(G_t,v)=0\implies Act(G_t,v)=0.
\end{equation}

\item \textbf{monotonicity}:

If a vertex $v$ performs an interaction $(v,u)$ at time $t$, and suppose the
interaction is the only change to the graph at time $t$, then the activity of
$v$ should enhance.

\begin{equation}
E_{t+1}=E_t\cup\{e(u,v,t,w)\}\implies Act(G_{t+1},v)>Act(G_{t},v).
\end{equation}


\item \textbf{attenuation}:

If the network keeps silent, the activity metric of any node $v$ should be
decreasing over time by intuition. Because there is a lower bound for activity
and we want
the attenuation process to be infinity long since a node changed the network
once in past time is potential to make another. So we make the decay rate
of activity for silent nodes to be also decreasing. However in our pure growing
model, the change of network structure must happen at integral time. So we
expand the definition
\begin{equation}
G_t=G_{\lfloor t \rfloor}\text{ and } E_t=E_{\lfloor t \rfloor},
\end{equation}
to make $Act(G,v)$ a well defined $\mathbb{R}\to [0,1]$ function.

\begin{equation}
todo
\end{equation}

\end{itemize}
\subsection{Design and Analysis}
% TODO
\section{Algorithms}
% TODO
\subsection{Maintaining the basic graph structure}
% TODO
\subsection{Historical Queries}
% TODO
\subsection{Computing Metrics}
% TODO
\subsection{Mining Significant Data}
% TODO
\subsection{Optimization and Parallelizability}
% TODO
\section{Experiment}

We have implemented two of the algorithms using C++ language.
The program maintains a dynamic graph which contains $n$ vertices and does not have
any edge at the beginning.
It support the following operations:
\begin{itemize}
	\item \textsc{Add $u,v$:} increase the time counter by one, and then add an edge
		between vertex $u$ and vertex $v$.
	\item \textsc{QueryDeg $t,u$:} ask for the degree of $u$ at time $t$.
	\item \textsc{QueryCon $t,u,v$:} ask that whether vertices $u,v$ is connected or not
		$u$ at time $t$.
\end{itemize}
Note that the time counter is initially zero.
Each operation cost $O(\log n)$ time in our implemented algorithm.

\subsection{Checking for Correctness}
We implemented a simple algorithm together which cost $O(\max\{n,m\})$ time for each operation,
	where $m$ is the number of edges before the operation.
We have generated several small test cases and compare the results between the simple program
	and our $O(\log n)$ program.
There is no difference found between outputs of the two programs.

\subsection{Checking for Time Complexity}
We have generated several huge test cases to test the running time of the program.
The following tabular shows the informations about the cases and the running results.
\begin{center} \begin{tabular}{|c|c|c|c|c|}
	\hline
	Case & Number of & Number of & Density of & Running \\
	ID & vertices & operations & query & time \\
	\hline
	1 & 100000 & 200000 & 0.5 & 0.504 \\
	\hline
	2 & 100000 & 200000 & 0.5 & 0.524 \\
	\hline
	3 & 100000 & 200000 & 0.5 & 0.516 \\
	\hline
	4 & 200000 & 400000 & 0.5 & 1.072 \\
	\hline
	5 & 200000 & 400000 & 0.5 & 1.092 \\
	\hline
	6 & 200000 & 400000 & 0.5 & 1.072 \\
	\hline
	7 & 500000 & 500000 & 0.5 & 1.464 \\
	\hline
	8 & 500000 & 500000 & 0.5 & 1.492 \\
	\hline
	9 & 500000 & 1000000 & 0.5 & 2.772 \\
	\hline
	10 & 500000 & 1000000 & 0.5 & 2.732 \\
	\hline
	11 & 500000 & 2000000 & 0.5 & 5.232 \\
	\hline
	12 & 500000 & 2000000 & 0.5 & 5.256 \\
	\hline
	13 & 1000000 & 2000000 & 0.5 & 5.652 \\
	\hline
	14 & 1000000 & 2000000 & 0.5 & 5.808 \\
	\hline
	15 & 1000000 & 3000000 & 0.5 & 8.417 \\
	\hline
\end{tabular} \end{center}
We can see that the running time grows as linear as number of operations,
	but grows much slower when number of vertices grows (as $\log$).

\section{Conclusion and Perspective}
% TODO
\nocite{*}
\bibliography{paper.bib}
\end{document}

% vim: set ts=8 sw=8 noet sts=8 tw=79 indentexpr= fdm=marker foldmarker=<<<,>>>:
